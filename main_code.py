# ===================== IMPORTS ===================== #
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from wordcloud import WordCloud
from pyvi.ViTokenizer import tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import pickle
import streamlit as st
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from category_encoders import CatBoostEncoder
from sklearn.manifold import TSNE
from PIL import Image
#############################################################
# ===================== CUSTOM CSS ===================== #
st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap');

    html, body, [class*="css"] {
        font-family: 'Poppins', sans-serif;
    }

    /* Title Style */
    .title-center {
        text-align: center;
        font-size: 40px !important;
        font-weight: 600 !important;
        color: #2C3E50;
        padding-bottom: 10px;
    }

    /* Header Gradient */
    .header {
        background: linear-gradient(90deg, #0062E6, #33AEFF);
        padding: 15px;
        border-radius: 10px;
        color: white;
        margin-bottom: 20px;
        text-align: center;
        font-size: 22px;
        font-weight: 600;
    }

    /* Card Style */
    .card {
        background-color: #ffffff;
        padding: 20px;
        border-radius: 12px;
        box-shadow: 0 4px 10px rgba(0,0,0,0.1);
        text-align: center;
        margin-bottom: 20px;
    }

    .card h3 {
        margin: 0;
        color: #2C3E50;
        font-weight: 600;
    }

    .card p {
        margin: 0;
        font-size: 24px;
        color: #2980B9;
        font-weight: 600;
    }

    /* Center Image */
    .center-img {
        display: flex;
        justify-content: center;
        margin-bottom: 20px;
    }
    
    /* FULL PAGE WIDTH */
    .block-container {
        max-width: 77% !important;
        padding-left: 2rem !important;
        padding-right: 2rem !important;
    }

    /* FIX COLUMN WIDTH */
    .css-1r6slb0, .css-12oz5g7 {
        flex: 1 !important;
    }
    </style>
""", unsafe_allow_html=True)

#############################################################
# ============================================
#  BACKEND H√ÄM DUY NH·∫§T (T·ªêI ∆ØU TO√ÄN B·ªò)
# ============================================

@st.cache_resource
def load_backend():
    import re
    from pyvi.ViTokenizer import tokenize
    from sklearn.feature_extraction.text import TfidfVectorizer
    import pandas as pd
    import pickle

    # ============================================================
    # DEFINE load_dict BEFORE USING IT  ‚ùó‚ùó‚ùó
    # ============================================================
    def load_dict(path):
        d = {}
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split("\t")
                if len(parts) == 2:
                    d[parts[0]] = parts[1]
        return d

    # ============================================================
    # BASE DIRECTORY
    # ============================================================
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))

    # ==== Load stopwords ====
    STOP_WORD_FILE = os.path.join(BASE_DIR, "vietnamese-stopwords.txt")
    with open(STOP_WORD_FILE, "r", encoding="utf-8") as f:
        stop_words = f.read().split("\n")

    # ==== Load dictionaries ====
    emoji_dict = load_dict(os.path.join(BASE_DIR, "emojicon.txt"))
    wrong_dict = load_dict(os.path.join(BASE_DIR, "wrong-word.txt"))

    # ============================================================
    # ==== Normalize text ====
    def normalize_text_light(text):
        text = str(text).lower()
        for k, v in emoji_dict.items():
            text = text.replace(k, f" {v} ")
        for k, v in wrong_dict.items():
            text = text.replace(k, f" {v} ")
        text = re.sub(r"[^\w\s]", " ", text)
        return re.sub(r"\s+", " ", text).strip()

    # ==== Remove stopwords ====
    def remove_stopwords(text):
        return " ".join([w for w in text.split() if w not in stop_words])

    # ==== Final preprocess ====
    def preprocess_text(text):
        text = normalize_text_light(text)
        text = remove_stopwords(text)
        return " ".join(tokenize(text))

    # ============================================
    #  Load Cleaned Data
    # ============================================
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))

    # ---- Load main dataset (fix path!) ----
    df = pd.read_excel(os.path.join(BASE_DIR, "PJ3_Data_motobikes_cleaned.xlsx"))
    df["id"] = range(len(df))


    # ============================================
    #  Ensure Content_wt_joined exists
    # ============================================
    if "Content_wt_joined" not in df.columns:
        df["Content_wt"] = df["Content"].apply(normalize_text_light).apply(remove_stopwords)
        df["Content_wt_joined"] = df["Content_wt"].apply(lambda x: " ".join(tokenize(x)))

    # Thay chu·ªói r·ªóng
    df.loc[df["Content_wt_joined"].str.strip() == "", "Content_wt_joined"] = df["Ti√™u ƒë·ªÅ"]

    # ============================================
    #  Load cosine similarity
    # ============================================
    with open(r"Cosine_similarity_matrix.pkl", "rb") as f:
        cosine_sim = pickle.load(f)

    # ============================================
    #  Build TF-IDF
    # ============================================
    vectorizer = TfidfVectorizer(
        analyzer="word",
        token_pattern=r"(?u)\b\w+\b",
        min_df=2
    )
    tfidf_matrix = vectorizer.fit_transform(df["Content_wt_joined"])

    # ============================================
    #  Return EVERYTHING
    # ============================================
    return (
        df,
        cosine_sim,
        vectorizer,
        tfidf_matrix,
        normalize_text_light,
        remove_stopwords,
        preprocess_text
    )


# ============================
#  G·ªåI H√ÄM BACKEND 1 L·∫¶N DUY NH·∫§T
# ============================
df, cosine_sim, vectorizer, tfidf_matrix, normalize_text_light, remove_stopwords, preprocess_text = load_backend()



#############################################################
# =========== RECOMMEND FUNCTIONS =========== #

def get_recommendations(id, cosine_sim=cosine_sim, nums=7):
    idx = df.index[df["id"] == id][0]
    scores = list(enumerate(cosine_sim[idx]))
    scores = sorted(scores, key=lambda x: x[1], reverse=True)[1:nums+1]

    indices = [i[0] for i in scores]
    res = df.iloc[indices][["id","Ti√™u ƒë·ªÅ","Th∆∞∆°ng hi·ªáu","Gi√°","S·ªë Km ƒë√£ ƒëi","ƒê·ªãa ch·ªâ"]].copy()
    res["Cosine_Similarity"] = [round(i[1],3) for i in scores]
    return res

def recommend_by_keyword(keyword, nums=7):
    keyword_clean = preprocess_text(keyword)
    if keyword_clean.strip()=="":
        return df.head(nums)

    keyword_vec = vectorizer.transform([keyword_clean])
    scores = cosine_similarity(keyword_vec, tfidf_matrix).flatten()

    if scores.max()==0:
        return df.head(nums)

    top_idx = scores.argsort()[::-1][:nums]
    res = df.iloc[top_idx][["id","Ti√™u ƒë·ªÅ","Th∆∞∆°ng hi·ªáu","Gi√°","Dung t√≠ch xe","S·ªë Km ƒë√£ ƒëi","ƒê·ªãa ch·ªâ"]]
    res["Cosine_Similarity"] = scores[top_idx]
    return res

#############################################################
# ===================== HEADER ===================== #
st.markdown("<div class='header'>Motorcycle Recommendation & Clustering Dashboard</div>",
            unsafe_allow_html=True)

# ===================== TITLE ===================== #
st.markdown("<h1 class='title-center'>An App with a Recommendation System and Clustering</h1>",
            unsafe_allow_html=True)

# ===================== IMAGE CENTER ===================== #
# st.image("Project_3/GUI_XeMayCu/xe_may_cu.jpg", width=450)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ===================== IMAGE CENTER ===================== #
image_path = os.path.join(BASE_DIR, "Logo_ChoTot.png")
st.image(image_path, width="stretch")

# ===================== 2 COLUMNS LAYOUT ===================== #
col1, col2 = st.columns([1, 1])

# T√≠nh t·ªïng s·ªë d·ªØ li·ªáu
total_items = len(df)

# T√≠nh s·ªë c·ª•m (n·∫øu ƒë√£ clustering)
if st.session_state.get("labels") is not None:
    cluster_count = len(np.unique(st.session_state.labels))
else:
    cluster_count = 0    # ho·∫∑c ƒë·∫∑t =4 n·∫øu b·∫°n d√πng s·ªë c·ª•m c·ªë ƒë·ªãnh

# CARD 1: Total Items
with col1:
    st.markdown(f"""
        <div class='card'>
            <h3>Total Items Processed</h3>
            <p>{total_items}</p>
        </div>
    """, unsafe_allow_html=True)

# CARD 2: Clusters Identified
with col2:
    st.markdown(f"""
        <div class='card'>
            <h3>Clusters Identified</h3>
            <p>4</p>
        </div>
    """, unsafe_allow_html=True)



# ===================== SIDEBAR ===================== #
st.sidebar.title("‚öô Menu")

menu = [
    "Home",
    "App Description",
    "Control panels",
    "Recommendation & Clustering",
    "Visualization",
    "Task assignment"
]

page = st.sidebar.radio("Go to:", menu)

# ======================= ROUTING ========================= #

# =============== PAGE: HOME =============== #
if page == "Home":
    st.subheader("üèçÔ∏è Welcome to the Motorcycle Analytics Dashboard")
    st.write("""
        Bussiness Problem: M·ªôt s√†n th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ (ho·∫∑c website rao v·∫∑t xe m√°y c≈© nh∆∞ Ch·ª£ T·ªët,...) ƒëang g·∫∑p 3 v·∫•n ƒë·ªÅ l·ªõn:
        - Ng∆∞·ªùi mua kh√≥ t√¨m ƒë√∫ng xe ph√π h·ª£p v√¨ s·ªë tin ƒëƒÉng l·ªõn nh∆∞ng h·ªá th·ªëng tr·∫£ k·∫øt qu·∫£ kh√¥ng th·ª±c s·ª± gi·ªëng v·ªõi nhu c·∫ßu.
        - Ng∆∞·ªùi mua kh√¥ng bi·∫øt m·ª©c gi√° n√†o l√† h·ª£p l√Ω c√≥ th·ªÉ c√πng m·ªôt m·∫´u xe nh∆∞ng gi√° dao ƒë·ªông r·∫•t m·∫°nh
        - Ng∆∞·ªùi b√°n kh√¥ng bi·∫øt nh√≥m kh√°ch h√†ng n√†o ph√π h·ª£p v·ªõi xe c·ªßa h·ªç ƒë·ªÉ t·ªëi ∆∞u ho√° vi·ªác ti·∫øp c·∫≠n kh√°ch h√†ng ti·ªÅm nƒÉng.
             
        ·ª®ng d·ª•ng n√†y cho ph√©p b·∫°n:
        - üîç T√¨m ki·∫øm xe t∆∞∆°ng t·ª± b·∫±ng Recommendation System  
        - üìä Th·ª±c hi·ªán ph√¢n c·ª•m d·ª±a v√†o nhi·ªÅu thu·ªôc t√≠nh  
        - üé® Tr·ª±c quan h√≥a d·ªØ li·ªáu d·ªÖ d√†ng  
    """)
    st.info("Ch·ªçn m·ª•c ·ªü thanh b√™n tr√°i ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

# =============== PAGE: APP DESCRIPTION =============== #
elif page == "App Description":
    st.subheader("üìò Gi·ªõi thi·ªáu ·ª®ng d·ª•ng")
    st.write("""
        ·ª®ng d·ª•ng ƒë∆∞·ª£c x√¢y d·ª±ng g·ªìm 2 module ch√≠nh:

        **1Ô∏è‚É£ Recommendation System**
        - T√¨m nh·ªØng xe m√°y gi·ªëng nh·∫•t d·ª±a v√†o ID, T·ª´ kh√≥a,‚Ä¶
        - S·ª≠ d·ª•ng TF-IDF + Cosine Similarity  
        - Cho ph√©p g·ª£i √Ω theo **ID** ho·∫∑c theo **Keyword**

        **2Ô∏è‚É£ Clustering**
        - Gom nh√≥m xe theo gi√°, h√£ng, dung t√≠ch, nƒÉm ƒëƒÉng k√Ω‚Ä¶
        - Thu·∫≠t to√°n h·ªó tr·ª£:
            - KMeans
            - Agglomerative
            - Gaussian Mixture Model
        - Gi·∫£m chi·ªÅu: PCA, t-SNE, UMAP

        **3Ô∏è‚É£ Visualization**
        - Wordcloud
        - Bi·ªÉu ƒë·ªì ph√¢n b·ªë gi√°
        - Countplot th∆∞∆°ng hi·ªáu  
    """)

# =============== PAGE: CONTROL PANELS =============== #
elif page == "Control panels":
    st.subheader("üõ† Control Panel Settings")
    st.write("C·∫•u h√¨nh chung cho app (tu·ª≥ ch·ªçn m·ªü r·ªông):")

    items = st.slider("S·ªë l∆∞·ª£ng items hi·ªÉn th·ªã", 5, 50, 10)
    show_price = st.checkbox("Hi·ªÉn th·ªã th√¥ng tin gi√°", True)
    show_brand = st.checkbox("Hi·ªÉn th·ªã th∆∞∆°ng hi·ªáu", True)

    st.success("C√†i ƒë·∫∑t ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng.")

# =============== PAGE: RECOMMENDATION =============== #
elif page == "Recommendation & Clustering":
    tab1, tab2 = st.tabs(["üîç Recommendation System", "üì¶ Clustering"])

    # TAB 1 - Recommendation
    with tab1:
        st.header("üîç Motorcycle Recommendation System")
        mode = st.radio("Ch·ªçn c√°ch g·ª£i √Ω:", ["Theo ID", "Theo Keyword"])

        # By ID
        if mode=="Theo ID":
            input_id = st.number_input("Nh·∫≠p ID xe:", min_value=0, max_value=len(df)-1, step=1)
            nums = st.slider("S·ªë l∆∞·ª£ng g·ª£i √Ω:", 3,20,7)

            if st.button("üîé Recommend by ID"):
                result = get_recommendations(int(input_id), nums=nums)
                st.dataframe(result)

                # Wordcloud FIXED
                text = " ".join(result["Ti√™u ƒë·ªÅ"].astype(str))
                wc = WordCloud(width=800, height=350, background_color="white").generate(text)
                fig, ax = plt.subplots(figsize=(8,4))
                ax.imshow(wc, interpolation="bilinear")
                ax.axis("off")
                st.pyplot(fig)

        # By Keyword
        if mode=="Theo Keyword":
            keyword = st.text_input("Nh·∫≠p t·ª´ kh√≥a:")
            nums = st.slider("S·ªë l∆∞·ª£ng g·ª£i √Ω:", 3,20,7)

            if st.button("üîé Recommend by Keyword"):
                result = recommend_by_keyword(keyword, nums)
                st.dataframe(result)

                # Wordcloud FIXED
                text = " ".join(result["Ti√™u ƒë·ªÅ"].astype(str))
                wc = WordCloud(width=800, height=350, background_color="white").generate(text)
                fig, ax = plt.subplots(figsize=(8,4))
                ax.imshow(wc, interpolation="bilinear")
                ax.axis("off")
                st.pyplot(fig)
    
    # ---------------- NAMES FOR CLUSTERS ----------------
    cluster_names = {
        0: "Xe cao c·∫•p",
        1: "Xe ph·ªï th√¥ng",
        2: "Xe t·∫ßm trung",
        3: "Xe gi√° r·∫ª"
    }

    # ---------------- INIT SESSION STATE ----------------
    for key in ["cluster_model", "labels", "encoder", "scaler", "df2_cluster", "X2_scaled"]:
        if key not in st.session_state:
            st.session_state[key] = None


    # ---------------- BUILD CLUSTER DATASET ----------------
    def build_cluster_dataset(df):
        """Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ clustering v·ªõi PowerTransformer"""
        features = ['Gi√°_num', 'Km_num', 'Dung_tich_num', 'NƒÉm ƒëƒÉng k√Ω', 'Th∆∞∆°ng hi·ªáu']
        df2 = df[features].dropna()

        # 1. Encode th∆∞∆°ng hi·ªáu b·∫±ng CatBoostEncoder
        encoder = CatBoostEncoder()
        brand_encoded = encoder.fit_transform(df2['Th∆∞∆°ng hi·ªáu'], df2['Gi√°_num'])

        # 2. Scale numeric b·∫±ng PowerTransformer
        scaler = PowerTransformer(method="yeo-johnson", standardize=True)
        numeric_scaled = scaler.fit_transform(
            df2[['Gi√°_num', 'Km_num', 'Dung_tich_num', 'NƒÉm ƒëƒÉng k√Ω']]
        )

        # 3. Combine numeric + brand encoding
        X = np.concatenate([numeric_scaled, brand_encoded.values], axis=1)

        return X, df2, encoder, scaler


    # ---------------- TAB 2: CLUSTERING ----------------
    with tab2:

        st.header("üì¶ Motorcycle Clustering cho b·ªô d·ªØ li·ªáu n√†y ƒë∆∞·ª£c chia l√†m 4 c·ª•m")

        algo = st.selectbox(
            "Ch·ªçn thu·∫≠t to√°n clustering",
            ["KMeans", "Gaussian Mixture", "Agglomerative"]
        )

        # ---------- RUN CLUSTERING ----------
        if st.button("üöÄ Ch·∫°y clustering"):
            X2_scaled, df2_cluster, encoder, scaler = build_cluster_dataset(df)

            # Ch·ªçn m√¥ h√¨nh
            if algo == "KMeans":
                model = KMeans(n_clusters=4, random_state=42)
                labels = model.fit_predict(X2_scaled)

            elif algo == "Gaussian Mixture":
                model = GaussianMixture(n_components=4, random_state=42)
                labels = model.fit_predict(X2_scaled)

            else:
                model = AgglomerativeClustering(n_clusters=4)
                labels = model.fit_predict(X2_scaled)

            # Silhouette Score
            sil = silhouette_score(X2_scaled, labels)

            # L∆∞u session_state
            st.session_state.cluster_model = model
            st.session_state.labels = labels
            st.session_state.X2_scaled = X2_scaled
            st.session_state.encoder = encoder
            st.session_state.scaler = scaler

            df2_cluster['Cluster'] = labels
            st.session_state.df2_cluster = df2_cluster

            st.success(f"üéØ ƒê√£ ph√¢n c·ª•m th√†nh c√¥ng b·∫±ng {algo} ‚Äî **Silhouette Score = {sil:.3f}**")
            st.dataframe(df2_cluster.head(20))

            # PCA Plot
            pca = PCA(n_components=2)
            comps = pca.fit_transform(X2_scaled)
            fig, ax = plt.subplots(figsize=(8, 5))
            sns.scatterplot(x=comps[:, 0], y=comps[:, 1], hue=labels, palette="tab10", ax=ax)
            st.pyplot(fig)

        # --------- CURRENT RESULTS ---------
        if st.session_state.cluster_model is not None:
            st.subheader("üìå K·∫øt qu·∫£ clustering hi·ªán t·∫°i")
            st.dataframe(st.session_state.df2_cluster.head(20))

        st.markdown("---")
        # ======== NAME CLUSTERS ========
        name_map = {
            0: "C·ª•m 0: Xe cao c·∫•p",
            1: "C·ª•m 1: Xe ph·ªï th√¥ng",
            2: "C·ª•m 2: Xe t·∫ßm trung",
            3: "C·ª•m 3: Xe gi√° r·∫ª"
        }

        st.write("T√™n c·ª•m g·ª£i √Ω:", name_map)
        # ---------------- D·ª∞ ƒêO√ÅN C·ª§M CHO XE M·ªöI ----------------
        st.subheader("üîÆ D·ª± ƒëo√°n c·ª•m cho xe m·ªõi")

        if st.session_state.cluster_model is None:
            st.warning("‚ö† B·∫°n c·∫ßn ch·∫°y clustering tr∆∞·ªõc!")
        else:
            gia = st.number_input("Gi√° xe", min_value=0)
            km = st.number_input("Km ƒë√£ ƒëi", min_value=0)
            cc = st.number_input("Dung t√≠ch (cc)", min_value=50)
            year = st.number_input("NƒÉm ƒëƒÉng k√Ω", min_value=1990, max_value=2025)
            brand = st.selectbox("Th∆∞∆°ng hi·ªáu", df['Th∆∞∆°ng hi·ªáu'].unique())

            if st.button("üîç Ph√¢n c·ª•m xe c·ªßa b·∫°n"):

                encoder = st.session_state.encoder
                scaler = st.session_state.scaler
                model = st.session_state.cluster_model

                # Encode brand
                new_brand = encoder.transform(pd.DataFrame({"Th∆∞∆°ng hi·ªáu": [brand]}))

                # Scale numeric b·∫±ng PowerTransformer
                new_numeric = scaler.transform([[gia, km, cc, year]])

                X_new = np.concatenate([new_numeric, new_brand.values], axis=1)

                # Predict
                if hasattr(model, "predict"):
                    cluster_id = model.predict(X_new)[0]
                else:
                    # Agglomerative fallback
                    centroids = np.vstack([
                        st.session_state.X2_scaled[st.session_state.labels == c].mean(axis=0)
                        for c in range(4)
                    ])
                    cluster_id = np.argmin(np.linalg.norm(centroids - X_new, axis=1))

                cluster_label = cluster_names.get(cluster_id, "Kh√¥ng r√µ")

                st.success(f"‚úî Xe c·ªßa b·∫°n thu·ªôc **C·ª•m {cluster_id} ‚Äì {cluster_label}!**")

# =============== PAGE: VISUALIZATION =============== #
elif page == "Visualization":
    st.subheader("üìä Visualization Dashboard")

    # Histogram
    fig1, ax1 = plt.subplots(figsize=(8,5))
    sns.histplot(df["Gi√°"].dropna(), kde=True, ax=ax1)
    st.pyplot(fig1)

    # WordCloud
    text = " ".join(df["Ti√™u ƒë·ªÅ"].astype(str))
    wc = WordCloud(width=900, height=400, background_color="white").generate(text)
    fig2, ax2 = plt.subplots(figsize=(9,4))
    ax2.imshow(wc, interpolation="bilinear")
    ax2.axis("off")
    st.pyplot(fig2)

# =============== PAGE: TASK ASSIGNMENT =============== #
elif page == "Task assignment":
    st.subheader("üìã Task Assignment")

    st.markdown("""
        ### üßë‚Äçüíª B·∫£ng ph√¢n c√¥ng c√¥ng vi·ªác
        
        | Th√†nh vi√™n         | C√¥ng vi·ªác |
        |--------------------|-----------|
        | **Nguy·ªÖn Duy Thanh** | GUI for Recommendation System and Clustering |
        | **Nguy·ªÖn Th√°i B√¨nh** | GUI for Price Prediction and Anomaly Detection |
    """)


# ===================== FOOTER ===================== #
st.sidebar.markdown("---")
# Load ·∫£nh
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ===================== AVATAR ===================== #
avatar_path = os.path.join(BASE_DIR, "avatar.jpg")
avatar = Image.open(avatar_path)

# --- TH√îNG S·ªê ---
offset_ratio = 0.10   # d·ªãch xu·ªëng 15% chi·ªÅu cao ·∫£nh (c√≥ th·ªÉ ch·ªânh 0.10‚Äì0.25)

# --- Crop top nh∆∞ng d·ªãch xu·ªëng ---
w, h = avatar.size
size = min(w, h)

# T√≠nh offset theo t·ªâ l·ªá chi·ªÅu cao
offset = int(size * offset_ratio)

left   = (w - size) / 2
top    = offset
right  = (w + size) / 2
bottom = offset + size

# ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° ·∫£nh th·∫≠t
bottom = min(bottom, h)

avatar = avatar.crop((left, top, right, bottom))

# --- Resize s·∫Øc n√©t ---
avatar = avatar.resize((80, 80), Image.LANCZOS)

# --- Hi·ªÉn th·ªã ---
st.sidebar.image(avatar, width=80, use_column_width=False)

# --- Footer ---
st.sidebar.write("Designed by **Duy-Thanh Nguyen**")
st.sidebar.write("Email: duythanh200620@gmail.com")
